Instruction for Workshop Setup K8S Multiple Cluster in RealWorld:
Note: This instruction will start lab for kubernetes's cluster for real workshop:

1. (Master Node) Install prerequiste software for operate:

	1.1 Check for module enable (IF not run script below for enable)
        sudo lsmod | grep -e ip_vs -e nf_conntrack_ipv4
	
    1.2 Check and reinstall cilium binary
    #Cilium Cli
    curl -L --remote-name-all https://github.com/cilium/cilium-cli/releases/latest/download/cilium-linux-amd64.tar.gz{,.sha256sum}
    sha256sum --check cilium-linux-amd64.tar.gz.sha256sum
    sudo tar xzvfC cilium-linux-amd64.tar.gz /usr/local/bin
    rm cilium-linux-amd64.tar.gz{,.sha256sum}

    #Hubble Cli
    export HUBBLE_VERSION=$(curl -s https://raw.githubusercontent.com/cilium/hubble/master/stable.txt)
    curl -L --remote-name-all https://github.com/cilium/hubble/releases/download/$HUBBLE_VERSION/hubble-linux-amd64.tar.gz{,.sha256sum}
    sha256sum --check hubble-linux-amd64.tar.gz.sha256sum
    sudo tar xzvfC hubble-linux-amd64.tar.gz /usr/local/bin
    rm hubble-linux-amd64.tar.gz{,.sha256sum}

    1.3: Install helm client on master node by command:
	curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3
   chmod 700 get_helm.sh
   ./get_helm.sh


2. Check "LAB" sheet for your group and inform your team for all node information like below:
====================================================
Cluster 1
Lab Description: (Check you excel sheet)
ClusterName: XXXXX
CA-Cert-Hash: XXXXX
Token: XXXXX
Ingress CNAME: XXXXX

Machine name		            			Roles:			IP Address: (Private)		IP Address: (Public)			Hostname
Training_DockerZerotoHero_StudentGX_1	   	Master			10.200.X.X					X.X.X.X							ip-10-200-X-X.ap-southeast-1.compute.internal
Training_DockerZerotoHero_StudentGX_2       NodePort		10.200.X.X					X.X.X.X							ip-10-200-X-X.ap-southeast-1.compute.internal
Training_DockerZerotoHero_StudentGX_3   	NodePort		10.200.X.X					X.X.X.X							ip-10-200-X-X.ap-southeast-1.compute.internal
===================================================
(export environment)
  export ClusterName=<clustername>
  export HostnameMaster1=<hostname master1>
  export HostnameWorker1=<hostname worker1>
  export HostnameWorker2=<hostname worker2>


  export tokenid=<tokenid>   #export this after master node had been initial
  export cacerhash=<CA-Cert-Hash>   #export this after master node had been initial
===================================================

====================================================
Cluster 2
Lab Description: (Check you excel sheet)
ClusterName: XXXXX
CA-Cert-Hash: XXXXX
Token: XXXXX
Ingress CNAME: XXXXX

Machine name		            			Roles:			IP Address: (Private)		IP Address: (Public)			Hostname
Training_DockerZerotoHero_StudentGX_1	   	Master			10.200.X.X					X.X.X.X							ip-10-200-X-X.ap-southeast-1.compute.internal
Training_DockerZerotoHero_StudentGX_2       NodePort		10.200.X.X					X.X.X.X							ip-10-200-X-X.ap-southeast-1.compute.internal
Training_DockerZerotoHero_StudentGX_3   	NodePort		10.200.X.X					X.X.X.X							ip-10-200-X-X.ap-southeast-1.compute.internal
===================================================
(export environment)
  export ClusterName=<clustername>
  export HostnameMaster1=<hostname master1>
  export HostnameWorker1=<hostname worker1>
  export HostnameWorker2=<hostname worker2>


  export tokenid=<tokenid>   #export this after master node had been initial
  export cacerhash=<CA-Cert-Hash>   #export this after master node had been initial
===================================================


4. (Master Cluster1) Prepare configuration for initial kubernetes master
    (run export environment)
	cd ~
	curl https://raw.githubusercontent.com/praparn/sourcesetup/master/kubernetes_initial/version124/kubeadm-init.yaml > ~/kubeadm-init.yaml
  	sed -i -e "s/2.2.2.2/$HostnameMaster1/g" ~/kubeadm-init.yaml
  	sed -i -e "s/hostnamemaster/$HostnameMaster1/g" ~/kubeadm-init.yaml
  	sed -i -e "s/KubernetesClusterName/$ClusterName/g" ~/kubeadm-init.yaml
  	more ~/kubeadm-init.yaml

5. (Master Cluster1) initial cluster by command:
	sudo su -
	kubeadm init --config /home/ubuntu/kubeadm-init.yaml
	exit

	*Remark: Need to record token Output
    -------------------------------------------------
    Token output:
    -------------------------------------------------

You can now join any number of control-plane nodes by copying certificate authorities
and service account keys on each node and then running the following as root:

  kubeadm join 10.0.1.67:6443 --token 1ogaw7.1dx1qg34qzi6ha5t \
	--discovery-token-ca-cert-hash sha256:XXXXXX \
	--control-plane 

Then you can join any number of worker nodes by running the following on each as root:

kubeadm join 10.0.1.67:6443 --token 1ogaw7.1dx1qg34qzi6ha5t \
	--discovery-token-ca-cert-hash sha256:XXXXXX 
	-------------------------------------------------

6. (Master Cluster1) Setup run cluster system by command (Regular User):
	mkdir -p $HOME/.kube
  	sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
	sudo chown $(id -u):$(id -g) $HOME/.kube/config
	kubectl taint nodes --all node-role.kubernetes.io/master-
    cp .kube/config /home/ubuntu/kubeconfig_cluster1

7. (Master Cluster1) Check IPVS mode on kube-proxy by command:
    kubectl get pods -n kube-system
    kubectl logs kube-proxy-<XXXX> -n kube-system

8 (Master Cluster1) Check IPVS policy by ipvsadm
	sudo ipvsadm
	-------------------------------------------------
	Example Result:
	-------------------------------------------------
IP Virtual Server version 1.2.1 (size=4096)
Prot LocalAddress:Port Scheduler Flags
  -> RemoteAddress:Port           Forward Weight ActiveConn InActConn
TCP  ip-10-96-0-1.ap-southeast-1. rr
  -> ip-10-0-1-191.ap-southeast-1 Masq    1      0          6         
TCP  ip-10-96-0-10.ap-southeast-1 rr
TCP  ip-10-96-0-10.ap-southeast-1 rr
UDP  ip-10-96-0-10.ap-southeast-1 rr
	-------------------------------------------------


9. (Master Cluster2) Prepare configuration for initial kubernetes master
    (run export environment)
	cd ~
	curl https://raw.githubusercontent.com/praparn/sourcesetup/master/kubernetes_initial/version124/kubeadm-init.yaml > ~/kubeadm-init.yaml
  	sed -i -e "s/2.2.2.2/$HostnameMaster1/g" ~/kubeadm-init.yaml
  	sed -i -e "s/hostnamemaster/$HostnameMaster1/g" ~/kubeadm-init.yaml
  	sed -i -e "s/KubernetesClusterName/$ClusterName/g" ~/kubeadm-init.yaml
  	more ~/kubeadm-init.yaml

10. (Master Cluster2) initial cluster by command:
	sudo su -
	kubeadm init --config /home/ubuntu/kubeadm-init.yaml
	exit

	*Remark: Need to record token Output
    -------------------------------------------------
    Token output:
    -------------------------------------------------

You can now join any number of control-plane nodes by copying certificate authorities
and service account keys on each node and then running the following as root:

  kubeadm join 10.0.1.67:6443 --token 1ogaw7.1dx1qg34qzi6ha5t \
	--discovery-token-ca-cert-hash sha256:XXXXXX \
	--control-plane 

Then you can join any number of worker nodes by running the following on each as root:

kubeadm join 10.0.1.67:6443 --token 1ogaw7.1dx1qg34qzi6ha5t \
	--discovery-token-ca-cert-hash sha256:XXXXXX 
	-------------------------------------------------

11. (Master Cluster2) Setup run cluster system by command (Regular User):
	mkdir -p $HOME/.kube
  	sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
	sudo chown $(id -u):$(id -g) $HOME/.kube/config
	kubectl taint nodes --all node-role.kubernetes.io/master-
    cp .kube/config /home/ubuntu/kubeconfig_cluster2

12. (Master Cluster2) Check IPVS mode on kube-proxy by command:
    kubectl get pods -n kube-system
    kubectl logs kube-proxy-<XXXX> -n kube-system

13. (Master Cluster2) Check IPVS policy by ipvsadm
	sudo ipvsadm
	-------------------------------------------------
	Example Result:
	-------------------------------------------------
IP Virtual Server version 1.2.1 (size=4096)
Prot LocalAddress:Port Scheduler Flags
  -> RemoteAddress:Port           Forward Weight ActiveConn InActConn
TCP  ip-10-96-0-1.ap-southeast-1. rr
  -> ip-10-0-1-191.ap-southeast-1 Masq    1      0          6         
TCP  ip-10-96-0-10.ap-southeast-1 rr
TCP  ip-10-96-0-10.ap-southeast-1 rr
UDP  ip-10-96-0-10.ap-southeast-1 rr
	-------------------------------------------------
14. (Local) Config file kubeconfig for context of $CLUSTER1 and $CLUSTER2
    scp -i "<key file>" ubuntu@<Public IP Cluster1>:/home/ubuntu/kubeconfig_cluster1 ./kubeconfig_cluster1
    scp -i "<key file>" ubuntu@<Public IP Cluster2>:/home/ubuntu/kubeconfig_cluster2 ./kubeconfig_cluster2
    Setup context for cluster1/cluster2 ==> ./kubeconfig_clustermesh
    scp -i "<key file>" ./kubeconfig_clustermesh ubuntu@<Public IP Cluster1>:/home/ubuntu/kubeconfig_clustermesh 
    scp -i "<key file>" ./kubeconfig_clustermesh ubuntu@<Public IP Cluster2>:/home/ubuntu/kubeconfig_clustermesh 

15. (Master Cluster1) copy kubeconfig and backup
    cd /home/ubuntu/
    cp .kube/config .kube/config_backup
    cp kubeconfig_clustermesh .kube/config

16. (Master Cluster2) copy kubeconfig and backup
    cd /home/ubuntu/
    cp .kube/config .kube/config_backup
    cp kubeconfig_clustermesh .kube/config

17. (Master Cluster1) Install cilium network with plugin by command:
    cilium install --cluster-name <cluster1 name> --cluster-id 1


18. (Master Cluster2) Install cilium network with plugin by command:
    cilium install --cluster-name <cluster2 name> --cluster-id 2 --context <context cluster2> --inherit-ca <context cluster1>
    

19. (Master Cluster1 & Master Cluster2) Verify get pods of kube-system
    kubectl get pods -n=kube-system
    cilium status

20. (Master Cluster1) Enable cilium cluster
    cilium clustermesh enable --context <context cluster1> --service-type NodePort
    cilium clustermesh enable --context <context cluster2> --service-type NodePort

21. (Master Cluster1 & Master Cluster2) Verify get pods of kube-system
    kubectl get pods -n=kube-system
    cilium status
    cilium clustermesh status --context <context cluster1>  --wait
    cilium clustermesh status --context <context cluster2>  --wait

22. (Master Cluster1) Test connection between cluster via command: 
    cilium clustermesh connect --context <context cluster1> --destination-context <context cluster2>
    cilium clustermesh status --context <context cluster1> --wait
    cilium connectivity test --context <context cluster1> --multi-cluster <context cluster2>

23. (Master Cluster1) 

22. (Master Cluster1 & Master Cluster2) Install cert-manager for kubernetes:
    kubectl apply -f https://github.com/jetstack/cert-manager/releases/download/v1.7.1/cert-manager.yaml
    watch kubectl get all -n=cert-manager